{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning SAM2 with Cityscapes dataset\n",
    "import os, sys\n",
    "import numpy as np\n",
    "KD_path = \"/home/avalocal/thesis23/KD\"\n",
    "sys.path.append(KD_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scratch:\\n  resolution: 1024\\n  train_batch_size: 1\\n  num_train_workers: 10\\n  num_frames: 8\\n  max_num_objects: 3\\n  base_lr: 5.0e-6\\n  vision_lr: 3.0e-06\\n  phases_per_epoch: 1\\n  num_epochs: 40'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''scratch:\n",
    "  resolution: 1024\n",
    "  train_batch_size: 1\n",
    "  num_train_workers: 10\n",
    "  num_frames: 8\n",
    "  max_num_objects: 3\n",
    "  base_lr: 5.0e-6\n",
    "  vision_lr: 3.0e-06\n",
    "  phases_per_epoch: 1\n",
    "  num_epochs: 40'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avalocal/miniconda3/envs/KD/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (None)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975\n"
     ]
    }
   ],
   "source": [
    "#dataset \n",
    "#images \n",
    "#annorations: labelIds\n",
    "#prompts: bboxes from labelIds (might consider or not)\n",
    "import os, sys, glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from cityscapes_original import CityscapesSV\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from loss_fns import MultiStepMultiMasksAndIous, sigmoid_focal_loss, dice_loss\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#dataset and dataloaders\n",
    "train_dataset = CityscapesSV(root_dir=\"/media/avalocal/T7/pardis/pardis/perception_system/datasets/cityscapes\", split='train')\n",
    "print(len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "train_batch_size = 4\n",
    "num_train_workers = 4\n",
    "\n",
    "# image, instance, ids,  bbox, num_objects, file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize_config_dir, compose\n",
    "from omegaconf import DictConfig\n",
    "import os\n",
    "#fine-tuning sam2\n",
    "ckpt_path = '/media/avalocal/T7/pardis/pardis/perception_system/ckpt/sam2.pth'\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SAM2Model(nn.Module):   #used for teacher model of depth prediction\n",
    "    def __init__(self):\n",
    "        super(SAM2Model, self).__init__()\n",
    "\n",
    "        # sam2_checkpoint = os.path.join(current_dir, \"sam2/checkpoints/sam2.1_hiera_large.pt\")\n",
    "        sam2_checkpoint = \"/home/avalocal/thesis23/KD/sam2/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "        model_cfg = \"sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "        config_dir = \"/home/avalocal/thesis23/KD/sam2/sam2/configs\"\n",
    "\n",
    "        with initialize_config_dir(version_base=None, config_dir=config_dir):\n",
    "            cfg = compose(config_name=model_cfg)\n",
    "            self.sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=DEVICE)\n",
    "            self.sam2_predictor = SAM2ImagePredictor(self.sam2_model)\n",
    "            self.sam2_predictor.model.sam_mask_decoder.train(True)\n",
    "            self.sam2_predictor.model.sam_prompt_encoder.train(True)\n",
    "        \n",
    "    def forward(self, x, points):\n",
    "        '''\n",
    "        input: x | should be np.array with size (h,w,3)\n",
    "        bbox: array w size (n, 4) where n is the number of bounding boxes\n",
    "        '''\n",
    "\n",
    "        # print(x.shape, bbox.shape)\n",
    "        self.sam2_predictor.set_image(x)\n",
    "\n",
    "        # prompt encoding\n",
    "        mask_input, unnorm_coords, labels, unnorm_box = self.sam2_predictor._prep_prompts(\n",
    "            points, None, None, box=None, mask_logits=None, normalize_coords=True)\n",
    "\n",
    "        if unnorm_coords is not None:\n",
    "            concat_points = (unnorm_coords, labels)\n",
    "        else:\n",
    "            concat_points = None\n",
    "\n",
    "        if unnorm_box is not None:\n",
    "            box_coords = unnorm_box.reshape(-1, 2 , 2) #n,4 -> n,2,2\n",
    "            box_labels = torch.tensor([[2, 3]], dtype=torch.int, device=unnorm_box.device)\n",
    "            box_labels = box_labels.repeat(unnorm_box.size(0), 1)\n",
    "            # we merge \"boxes\" and \"points\" into a single \"concat_points\" input (where\n",
    "            # boxes are added at the beginning) to sam_prompt_encoder\n",
    "            if concat_points is not None:\n",
    "                concat_coords = torch.cat([box_coords, concat_points[0]], dim=1)\n",
    "                concat_labels = torch.cat([box_labels, concat_points[1]], dim=1)\n",
    "                concat_points = (concat_coords, concat_labels)\n",
    "            else:\n",
    "                concat_points = (box_coords, box_labels)\n",
    "\n",
    "        \n",
    "        sparse_embeddings, dense_embeddings = self.sam2_predictor.model.sam_prompt_encoder(\n",
    "            points=concat_points,\n",
    "            boxes = None,\n",
    "            masks = mask_input,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # mask decoder\n",
    "\n",
    "        batched_mode = (\n",
    "            concat_points is not None and concat_points[0].shape[0] > 1\n",
    "        )  # multi object prediction\n",
    "        high_res_features = [\n",
    "            feat_level[-1].unsqueeze(0)\n",
    "            for feat_level in self.sam2_predictor._features[\"high_res_feats\"]\n",
    "        ]\n",
    "        low_res_masks, iou_predictions, _, _ = self.sam2_predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=self.sam2_predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=self.sam2_predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            repeat_image=batched_mode,\n",
    "            high_res_features=high_res_features,\n",
    "        )\n",
    "\n",
    "        # Upscale the masks to the original image resolution\n",
    "        masks = self.sam2_predictor._transforms.postprocess_masks(\n",
    "            low_res_masks, self.sam2_predictor._orig_hw[-1]\n",
    "        )\n",
    "\n",
    "        # print(masks.shape, \"this should be output\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # masks, scores, logits = self.sam2_predictor.predict(\n",
    "        #                     point_coords=None,\n",
    "        #                     point_labels=None,\n",
    "        #                     box=bbox,\n",
    "        #                     multimask_output=False,\n",
    "        #                 )\n",
    "        # print(masks.shape, scores.shape, logits.shape, \"this should be output\")\n",
    "\n",
    "        return masks\n",
    "\n",
    "\n",
    "# criterion = MultiStepMultiMasksAndIous(weight_dict={\"loss_mask\": 20.0, \"loss_dice\": 1.0, \"loss_iou\": 1.0})\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [11:52<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 35.176852345426546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [11:39<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 27.81259161644623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [11:33<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 23.64198478318062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [11:41<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 19.787373292826803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2975/2975 [11:52<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 16.219683913663655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2975 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#train loop\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#finetuning for cityscapes for semantic segmentation\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, DEVICE, epoch):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        img, _, _, _, _,labels, file_name = batch\n",
    "\n",
    "        img *= 255.0\n",
    "        img = F.interpolate(img, size=(1024, 1024), mode='bilinear', align_corners=False) #1, 3, 1024, 2048\n",
    "        img = img[0].permute(1, 2, 0).numpy().copy().astype(np.uint8)\n",
    "\n",
    "        labels = F.interpolate(labels, size=(1024, 1024), mode='nearest')\n",
    "        labels = labels[0].numpy().copy().astype(np.uint8) #1, 1024, 1024\n",
    "        #1, 1024, 1024 -> N, 1024, 1024\n",
    "        labels_ = np.zeros((19, 1024, 1024), dtype=np.uint8)\n",
    "        preds_ = np.zeros((19, 1024, 1024), dtype=np.uint8)\n",
    "        for i in range(19):\n",
    "            labels_[i] = (labels==i).astype(np.uint8)\n",
    "\n",
    "        # print(img.shape, instance.shape, ids.shape, bbox.shape, num_objects, file_name)\n",
    "        #([1, 3, 1024, 2048]) ([1, 100, 1024, 2048]) ([1, 100]) ([1, 100, 4]) ([19]) ('weimar_000091_000019_leftImg8bit.png',)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "\n",
    "        for cls in range(len(19)):\n",
    "\n",
    "            #pick N = 5 random points from mask==cls\n",
    "            mask = labels == cls\n",
    "            mask = mask.astype(np.uint8)\n",
    "\n",
    "            #pick N = 5 random points from mask==cls\n",
    "            points = np.argwhere(mask)\n",
    "            np.random.shuffle(points)\n",
    "            points = points[:5]\n",
    "\n",
    "            pred_mask = model(img, points) #N 1 1024 1024\n",
    "\n",
    "            if pred_mask.shape[0]>1: \n",
    "                #N, 1, 1024, 1024 -> 1, 1, 1024, 1024 \n",
    "                pred_mask, _ = torch.max(pred_mask, dim=0)\n",
    "                #pred_mask.max(dim=0, keepdim=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            focalLoss = sigmoid_focal_loss(pred_mask, mask, 1, loss_on_multimask=False)\n",
    "            diceLoss = dice_loss(pred_mask, mask, 1, loss_on_multimask=False)\n",
    "\n",
    "\n",
    "            loss += 20.0 * focalLoss + 1.0 * diceLoss\n",
    "\n",
    "        # focalLoss = sigmoid_focal_loss(masks, instance, num_objects[0], loss_on_multimask=False)\n",
    "        # diceLoss = dice_loss(masks, instance, num_objects[0], loss_on_multimask=False)\n",
    "        # loss = 20 * focalLoss + 1 * diceLoss\n",
    "\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss/len(train_loader)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "model = SAM2Model()\n",
    "model = model.to(DEVICE)\n",
    "##image_encoder, memory_attention, memory_encoder, sam_prompt_encoder, sam_mask_decoder\n",
    "#freeze all layers except sam_prompt_encoder and sam_mask_decoder\n",
    "for name, param in model.named_parameters():\n",
    "    if 'sam_prompt_encoder' in name or 'sam_mask_decoder' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=5.0e-6, weight_decay=1e-4, betas=(0.9, 0.999))\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(optimizer, steps_per_epoch * num_epochs, power=1.0) #mask2former paper\n",
    "\n",
    "        \n",
    "\n",
    "#train loop\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler, DEVICE, epoch)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "KD_path = \"/home/avalocal/thesis23/KD\"\n",
    "sys.path.append(KD_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.)\n",
      "odict_keys(['module.image_encoder.trunk.pos_embed', 'module.image_encoder.trunk.pos_embed_window', 'module.image_encoder.trunk.patch_embed.proj.weight', 'module.image_encoder.trunk.patch_embed.proj.bias', 'module.image_encoder.trunk.blocks.0.norm1.weight', 'module.image_encoder.trunk.blocks.0.norm1.bias', 'module.image_encoder.trunk.blocks.0.attn.qkv.weight', 'module.image_encoder.trunk.blocks.0.attn.qkv.bias', 'module.image_encoder.trunk.blocks.0.attn.proj.weight', 'module.image_encoder.trunk.blocks.0.attn.proj.bias', 'module.image_encoder.trunk.blocks.0.norm2.weight', 'module.image_encoder.trunk.blocks.0.norm2.bias', 'module.image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.1.norm1.weight', 'module.image_encoder.trunk.blocks.1.norm1.bias', 'module.image_encoder.trunk.blocks.1.attn.qkv.weight', 'module.image_encoder.trunk.blocks.1.attn.qkv.bias', 'module.image_encoder.trunk.blocks.1.attn.proj.weight', 'module.image_encoder.trunk.blocks.1.attn.proj.bias', 'module.image_encoder.trunk.blocks.1.norm2.weight', 'module.image_encoder.trunk.blocks.1.norm2.bias', 'module.image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.2.norm1.weight', 'module.image_encoder.trunk.blocks.2.norm1.bias', 'module.image_encoder.trunk.blocks.2.attn.qkv.weight', 'module.image_encoder.trunk.blocks.2.attn.qkv.bias', 'module.image_encoder.trunk.blocks.2.attn.proj.weight', 'module.image_encoder.trunk.blocks.2.attn.proj.bias', 'module.image_encoder.trunk.blocks.2.norm2.weight', 'module.image_encoder.trunk.blocks.2.norm2.bias', 'module.image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.2.proj.weight', 'module.image_encoder.trunk.blocks.2.proj.bias', 'module.image_encoder.trunk.blocks.3.norm1.weight', 'module.image_encoder.trunk.blocks.3.norm1.bias', 'module.image_encoder.trunk.blocks.3.attn.qkv.weight', 'module.image_encoder.trunk.blocks.3.attn.qkv.bias', 'module.image_encoder.trunk.blocks.3.attn.proj.weight', 'module.image_encoder.trunk.blocks.3.attn.proj.bias', 'module.image_encoder.trunk.blocks.3.norm2.weight', 'module.image_encoder.trunk.blocks.3.norm2.bias', 'module.image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.4.norm1.weight', 'module.image_encoder.trunk.blocks.4.norm1.bias', 'module.image_encoder.trunk.blocks.4.attn.qkv.weight', 'module.image_encoder.trunk.blocks.4.attn.qkv.bias', 'module.image_encoder.trunk.blocks.4.attn.proj.weight', 'module.image_encoder.trunk.blocks.4.attn.proj.bias', 'module.image_encoder.trunk.blocks.4.norm2.weight', 'module.image_encoder.trunk.blocks.4.norm2.bias', 'module.image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.5.norm1.weight', 'module.image_encoder.trunk.blocks.5.norm1.bias', 'module.image_encoder.trunk.blocks.5.attn.qkv.weight', 'module.image_encoder.trunk.blocks.5.attn.qkv.bias', 'module.image_encoder.trunk.blocks.5.attn.proj.weight', 'module.image_encoder.trunk.blocks.5.attn.proj.bias', 'module.image_encoder.trunk.blocks.5.norm2.weight', 'module.image_encoder.trunk.blocks.5.norm2.bias', 'module.image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.5.proj.weight', 'module.image_encoder.trunk.blocks.5.proj.bias', 'module.image_encoder.trunk.blocks.6.norm1.weight', 'module.image_encoder.trunk.blocks.6.norm1.bias', 'module.image_encoder.trunk.blocks.6.attn.qkv.weight', 'module.image_encoder.trunk.blocks.6.attn.qkv.bias', 'module.image_encoder.trunk.blocks.6.attn.proj.weight', 'module.image_encoder.trunk.blocks.6.attn.proj.bias', 'module.image_encoder.trunk.blocks.6.norm2.weight', 'module.image_encoder.trunk.blocks.6.norm2.bias', 'module.image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.7.norm1.weight', 'module.image_encoder.trunk.blocks.7.norm1.bias', 'module.image_encoder.trunk.blocks.7.attn.qkv.weight', 'module.image_encoder.trunk.blocks.7.attn.qkv.bias', 'module.image_encoder.trunk.blocks.7.attn.proj.weight', 'module.image_encoder.trunk.blocks.7.attn.proj.bias', 'module.image_encoder.trunk.blocks.7.norm2.weight', 'module.image_encoder.trunk.blocks.7.norm2.bias', 'module.image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.8.norm1.weight', 'module.image_encoder.trunk.blocks.8.norm1.bias', 'module.image_encoder.trunk.blocks.8.attn.qkv.weight', 'module.image_encoder.trunk.blocks.8.attn.qkv.bias', 'module.image_encoder.trunk.blocks.8.attn.proj.weight', 'module.image_encoder.trunk.blocks.8.attn.proj.bias', 'module.image_encoder.trunk.blocks.8.norm2.weight', 'module.image_encoder.trunk.blocks.8.norm2.bias', 'module.image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.9.norm1.weight', 'module.image_encoder.trunk.blocks.9.norm1.bias', 'module.image_encoder.trunk.blocks.9.attn.qkv.weight', 'module.image_encoder.trunk.blocks.9.attn.qkv.bias', 'module.image_encoder.trunk.blocks.9.attn.proj.weight', 'module.image_encoder.trunk.blocks.9.attn.proj.bias', 'module.image_encoder.trunk.blocks.9.norm2.weight', 'module.image_encoder.trunk.blocks.9.norm2.bias', 'module.image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.10.norm1.weight', 'module.image_encoder.trunk.blocks.10.norm1.bias', 'module.image_encoder.trunk.blocks.10.attn.qkv.weight', 'module.image_encoder.trunk.blocks.10.attn.qkv.bias', 'module.image_encoder.trunk.blocks.10.attn.proj.weight', 'module.image_encoder.trunk.blocks.10.attn.proj.bias', 'module.image_encoder.trunk.blocks.10.norm2.weight', 'module.image_encoder.trunk.blocks.10.norm2.bias', 'module.image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.11.norm1.weight', 'module.image_encoder.trunk.blocks.11.norm1.bias', 'module.image_encoder.trunk.blocks.11.attn.qkv.weight', 'module.image_encoder.trunk.blocks.11.attn.qkv.bias', 'module.image_encoder.trunk.blocks.11.attn.proj.weight', 'module.image_encoder.trunk.blocks.11.attn.proj.bias', 'module.image_encoder.trunk.blocks.11.norm2.weight', 'module.image_encoder.trunk.blocks.11.norm2.bias', 'module.image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.12.norm1.weight', 'module.image_encoder.trunk.blocks.12.norm1.bias', 'module.image_encoder.trunk.blocks.12.attn.qkv.weight', 'module.image_encoder.trunk.blocks.12.attn.qkv.bias', 'module.image_encoder.trunk.blocks.12.attn.proj.weight', 'module.image_encoder.trunk.blocks.12.attn.proj.bias', 'module.image_encoder.trunk.blocks.12.norm2.weight', 'module.image_encoder.trunk.blocks.12.norm2.bias', 'module.image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.13.norm1.weight', 'module.image_encoder.trunk.blocks.13.norm1.bias', 'module.image_encoder.trunk.blocks.13.attn.qkv.weight', 'module.image_encoder.trunk.blocks.13.attn.qkv.bias', 'module.image_encoder.trunk.blocks.13.attn.proj.weight', 'module.image_encoder.trunk.blocks.13.attn.proj.bias', 'module.image_encoder.trunk.blocks.13.norm2.weight', 'module.image_encoder.trunk.blocks.13.norm2.bias', 'module.image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.14.norm1.weight', 'module.image_encoder.trunk.blocks.14.norm1.bias', 'module.image_encoder.trunk.blocks.14.attn.qkv.weight', 'module.image_encoder.trunk.blocks.14.attn.qkv.bias', 'module.image_encoder.trunk.blocks.14.attn.proj.weight', 'module.image_encoder.trunk.blocks.14.attn.proj.bias', 'module.image_encoder.trunk.blocks.14.norm2.weight', 'module.image_encoder.trunk.blocks.14.norm2.bias', 'module.image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.15.norm1.weight', 'module.image_encoder.trunk.blocks.15.norm1.bias', 'module.image_encoder.trunk.blocks.15.attn.qkv.weight', 'module.image_encoder.trunk.blocks.15.attn.qkv.bias', 'module.image_encoder.trunk.blocks.15.attn.proj.weight', 'module.image_encoder.trunk.blocks.15.attn.proj.bias', 'module.image_encoder.trunk.blocks.15.norm2.weight', 'module.image_encoder.trunk.blocks.15.norm2.bias', 'module.image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.16.norm1.weight', 'module.image_encoder.trunk.blocks.16.norm1.bias', 'module.image_encoder.trunk.blocks.16.attn.qkv.weight', 'module.image_encoder.trunk.blocks.16.attn.qkv.bias', 'module.image_encoder.trunk.blocks.16.attn.proj.weight', 'module.image_encoder.trunk.blocks.16.attn.proj.bias', 'module.image_encoder.trunk.blocks.16.norm2.weight', 'module.image_encoder.trunk.blocks.16.norm2.bias', 'module.image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.17.norm1.weight', 'module.image_encoder.trunk.blocks.17.norm1.bias', 'module.image_encoder.trunk.blocks.17.attn.qkv.weight', 'module.image_encoder.trunk.blocks.17.attn.qkv.bias', 'module.image_encoder.trunk.blocks.17.attn.proj.weight', 'module.image_encoder.trunk.blocks.17.attn.proj.bias', 'module.image_encoder.trunk.blocks.17.norm2.weight', 'module.image_encoder.trunk.blocks.17.norm2.bias', 'module.image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.18.norm1.weight', 'module.image_encoder.trunk.blocks.18.norm1.bias', 'module.image_encoder.trunk.blocks.18.attn.qkv.weight', 'module.image_encoder.trunk.blocks.18.attn.qkv.bias', 'module.image_encoder.trunk.blocks.18.attn.proj.weight', 'module.image_encoder.trunk.blocks.18.attn.proj.bias', 'module.image_encoder.trunk.blocks.18.norm2.weight', 'module.image_encoder.trunk.blocks.18.norm2.bias', 'module.image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.19.norm1.weight', 'module.image_encoder.trunk.blocks.19.norm1.bias', 'module.image_encoder.trunk.blocks.19.attn.qkv.weight', 'module.image_encoder.trunk.blocks.19.attn.qkv.bias', 'module.image_encoder.trunk.blocks.19.attn.proj.weight', 'module.image_encoder.trunk.blocks.19.attn.proj.bias', 'module.image_encoder.trunk.blocks.19.norm2.weight', 'module.image_encoder.trunk.blocks.19.norm2.bias', 'module.image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.20.norm1.weight', 'module.image_encoder.trunk.blocks.20.norm1.bias', 'module.image_encoder.trunk.blocks.20.attn.qkv.weight', 'module.image_encoder.trunk.blocks.20.attn.qkv.bias', 'module.image_encoder.trunk.blocks.20.attn.proj.weight', 'module.image_encoder.trunk.blocks.20.attn.proj.bias', 'module.image_encoder.trunk.blocks.20.norm2.weight', 'module.image_encoder.trunk.blocks.20.norm2.bias', 'module.image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.21.norm1.weight', 'module.image_encoder.trunk.blocks.21.norm1.bias', 'module.image_encoder.trunk.blocks.21.attn.qkv.weight', 'module.image_encoder.trunk.blocks.21.attn.qkv.bias', 'module.image_encoder.trunk.blocks.21.attn.proj.weight', 'module.image_encoder.trunk.blocks.21.attn.proj.bias', 'module.image_encoder.trunk.blocks.21.norm2.weight', 'module.image_encoder.trunk.blocks.21.norm2.bias', 'module.image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.21.proj.weight', 'module.image_encoder.trunk.blocks.21.proj.bias', 'module.image_encoder.trunk.blocks.22.norm1.weight', 'module.image_encoder.trunk.blocks.22.norm1.bias', 'module.image_encoder.trunk.blocks.22.attn.qkv.weight', 'module.image_encoder.trunk.blocks.22.attn.qkv.bias', 'module.image_encoder.trunk.blocks.22.attn.proj.weight', 'module.image_encoder.trunk.blocks.22.attn.proj.bias', 'module.image_encoder.trunk.blocks.22.norm2.weight', 'module.image_encoder.trunk.blocks.22.norm2.bias', 'module.image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'module.image_encoder.trunk.blocks.23.norm1.weight', 'module.image_encoder.trunk.blocks.23.norm1.bias', 'module.image_encoder.trunk.blocks.23.attn.qkv.weight', 'module.image_encoder.trunk.blocks.23.attn.qkv.bias', 'module.image_encoder.trunk.blocks.23.attn.proj.weight', 'module.image_encoder.trunk.blocks.23.attn.proj.bias', 'module.image_encoder.trunk.blocks.23.norm2.weight', 'module.image_encoder.trunk.blocks.23.norm2.bias', 'module.image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'module.image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'module.image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'module.image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'module.image_encoder.neck.convs.0.conv.weight', 'module.image_encoder.neck.convs.0.conv.bias', 'module.image_encoder.neck.convs.1.conv.weight', 'module.image_encoder.neck.convs.1.conv.bias', 'module.image_encoder.neck.convs.2.conv.weight', 'module.image_encoder.neck.convs.2.conv.bias', 'module.image_encoder.neck.convs.3.conv.weight', 'module.image_encoder.neck.convs.3.conv.bias', 'module.model.image_encoder.trunk.pos_embed', 'module.model.image_encoder.trunk.pos_embed_window', 'module.model.image_encoder.trunk.patch_embed.proj.weight', 'module.model.image_encoder.trunk.patch_embed.proj.bias', 'module.model.image_encoder.trunk.blocks.0.norm1.weight', 'module.model.image_encoder.trunk.blocks.0.norm1.bias', 'module.model.image_encoder.trunk.blocks.0.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.0.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.0.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.0.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.0.norm2.weight', 'module.model.image_encoder.trunk.blocks.0.norm2.bias', 'module.model.image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.1.norm1.weight', 'module.model.image_encoder.trunk.blocks.1.norm1.bias', 'module.model.image_encoder.trunk.blocks.1.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.1.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.1.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.1.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.1.norm2.weight', 'module.model.image_encoder.trunk.blocks.1.norm2.bias', 'module.model.image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.2.norm1.weight', 'module.model.image_encoder.trunk.blocks.2.norm1.bias', 'module.model.image_encoder.trunk.blocks.2.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.2.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.2.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.2.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.2.norm2.weight', 'module.model.image_encoder.trunk.blocks.2.norm2.bias', 'module.model.image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.2.proj.weight', 'module.model.image_encoder.trunk.blocks.2.proj.bias', 'module.model.image_encoder.trunk.blocks.3.norm1.weight', 'module.model.image_encoder.trunk.blocks.3.norm1.bias', 'module.model.image_encoder.trunk.blocks.3.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.3.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.3.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.3.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.3.norm2.weight', 'module.model.image_encoder.trunk.blocks.3.norm2.bias', 'module.model.image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.4.norm1.weight', 'module.model.image_encoder.trunk.blocks.4.norm1.bias', 'module.model.image_encoder.trunk.blocks.4.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.4.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.4.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.4.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.4.norm2.weight', 'module.model.image_encoder.trunk.blocks.4.norm2.bias', 'module.model.image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.5.norm1.weight', 'module.model.image_encoder.trunk.blocks.5.norm1.bias', 'module.model.image_encoder.trunk.blocks.5.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.5.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.5.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.5.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.5.norm2.weight', 'module.model.image_encoder.trunk.blocks.5.norm2.bias', 'module.model.image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.5.proj.weight', 'module.model.image_encoder.trunk.blocks.5.proj.bias', 'module.model.image_encoder.trunk.blocks.6.norm1.weight', 'module.model.image_encoder.trunk.blocks.6.norm1.bias', 'module.model.image_encoder.trunk.blocks.6.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.6.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.6.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.6.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.6.norm2.weight', 'module.model.image_encoder.trunk.blocks.6.norm2.bias', 'module.model.image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.7.norm1.weight', 'module.model.image_encoder.trunk.blocks.7.norm1.bias', 'module.model.image_encoder.trunk.blocks.7.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.7.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.7.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.7.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.7.norm2.weight', 'module.model.image_encoder.trunk.blocks.7.norm2.bias', 'module.model.image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.8.norm1.weight', 'module.model.image_encoder.trunk.blocks.8.norm1.bias', 'module.model.image_encoder.trunk.blocks.8.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.8.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.8.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.8.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.8.norm2.weight', 'module.model.image_encoder.trunk.blocks.8.norm2.bias', 'module.model.image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.9.norm1.weight', 'module.model.image_encoder.trunk.blocks.9.norm1.bias', 'module.model.image_encoder.trunk.blocks.9.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.9.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.9.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.9.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.9.norm2.weight', 'module.model.image_encoder.trunk.blocks.9.norm2.bias', 'module.model.image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.10.norm1.weight', 'module.model.image_encoder.trunk.blocks.10.norm1.bias', 'module.model.image_encoder.trunk.blocks.10.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.10.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.10.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.10.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.10.norm2.weight', 'module.model.image_encoder.trunk.blocks.10.norm2.bias', 'module.model.image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.11.norm1.weight', 'module.model.image_encoder.trunk.blocks.11.norm1.bias', 'module.model.image_encoder.trunk.blocks.11.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.11.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.11.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.11.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.11.norm2.weight', 'module.model.image_encoder.trunk.blocks.11.norm2.bias', 'module.model.image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.12.norm1.weight', 'module.model.image_encoder.trunk.blocks.12.norm1.bias', 'module.model.image_encoder.trunk.blocks.12.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.12.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.12.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.12.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.12.norm2.weight', 'module.model.image_encoder.trunk.blocks.12.norm2.bias', 'module.model.image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.13.norm1.weight', 'module.model.image_encoder.trunk.blocks.13.norm1.bias', 'module.model.image_encoder.trunk.blocks.13.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.13.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.13.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.13.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.13.norm2.weight', 'module.model.image_encoder.trunk.blocks.13.norm2.bias', 'module.model.image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.14.norm1.weight', 'module.model.image_encoder.trunk.blocks.14.norm1.bias', 'module.model.image_encoder.trunk.blocks.14.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.14.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.14.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.14.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.14.norm2.weight', 'module.model.image_encoder.trunk.blocks.14.norm2.bias', 'module.model.image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.15.norm1.weight', 'module.model.image_encoder.trunk.blocks.15.norm1.bias', 'module.model.image_encoder.trunk.blocks.15.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.15.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.15.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.15.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.15.norm2.weight', 'module.model.image_encoder.trunk.blocks.15.norm2.bias', 'module.model.image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.16.norm1.weight', 'module.model.image_encoder.trunk.blocks.16.norm1.bias', 'module.model.image_encoder.trunk.blocks.16.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.16.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.16.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.16.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.16.norm2.weight', 'module.model.image_encoder.trunk.blocks.16.norm2.bias', 'module.model.image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.17.norm1.weight', 'module.model.image_encoder.trunk.blocks.17.norm1.bias', 'module.model.image_encoder.trunk.blocks.17.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.17.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.17.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.17.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.17.norm2.weight', 'module.model.image_encoder.trunk.blocks.17.norm2.bias', 'module.model.image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.18.norm1.weight', 'module.model.image_encoder.trunk.blocks.18.norm1.bias', 'module.model.image_encoder.trunk.blocks.18.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.18.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.18.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.18.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.18.norm2.weight', 'module.model.image_encoder.trunk.blocks.18.norm2.bias', 'module.model.image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.19.norm1.weight', 'module.model.image_encoder.trunk.blocks.19.norm1.bias', 'module.model.image_encoder.trunk.blocks.19.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.19.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.19.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.19.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.19.norm2.weight', 'module.model.image_encoder.trunk.blocks.19.norm2.bias', 'module.model.image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.20.norm1.weight', 'module.model.image_encoder.trunk.blocks.20.norm1.bias', 'module.model.image_encoder.trunk.blocks.20.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.20.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.20.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.20.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.20.norm2.weight', 'module.model.image_encoder.trunk.blocks.20.norm2.bias', 'module.model.image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.21.norm1.weight', 'module.model.image_encoder.trunk.blocks.21.norm1.bias', 'module.model.image_encoder.trunk.blocks.21.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.21.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.21.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.21.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.21.norm2.weight', 'module.model.image_encoder.trunk.blocks.21.norm2.bias', 'module.model.image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.21.proj.weight', 'module.model.image_encoder.trunk.blocks.21.proj.bias', 'module.model.image_encoder.trunk.blocks.22.norm1.weight', 'module.model.image_encoder.trunk.blocks.22.norm1.bias', 'module.model.image_encoder.trunk.blocks.22.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.22.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.22.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.22.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.22.norm2.weight', 'module.model.image_encoder.trunk.blocks.22.norm2.bias', 'module.model.image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'module.model.image_encoder.trunk.blocks.23.norm1.weight', 'module.model.image_encoder.trunk.blocks.23.norm1.bias', 'module.model.image_encoder.trunk.blocks.23.attn.qkv.weight', 'module.model.image_encoder.trunk.blocks.23.attn.qkv.bias', 'module.model.image_encoder.trunk.blocks.23.attn.proj.weight', 'module.model.image_encoder.trunk.blocks.23.attn.proj.bias', 'module.model.image_encoder.trunk.blocks.23.norm2.weight', 'module.model.image_encoder.trunk.blocks.23.norm2.bias', 'module.model.image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'module.model.image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'module.model.image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'module.model.image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'module.model.image_encoder.neck.convs.0.conv.weight', 'module.model.image_encoder.neck.convs.0.conv.bias', 'module.model.image_encoder.neck.convs.1.conv.weight', 'module.model.image_encoder.neck.convs.1.conv.bias', 'module.model.image_encoder.neck.convs.2.conv.weight', 'module.model.image_encoder.neck.convs.2.conv.bias', 'module.model.image_encoder.neck.convs.3.conv.weight', 'module.model.image_encoder.neck.convs.3.conv.bias', 'module.model.mask_decoder.transformer.layers.0.self_attn.q_proj.weight', 'module.model.mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'module.model.mask_decoder.transformer.layers.0.self_attn.k_proj.weight', 'module.model.mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'module.model.mask_decoder.transformer.layers.0.self_attn.v_proj.weight', 'module.model.mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'module.model.mask_decoder.transformer.layers.0.self_attn.out_proj.weight', 'module.model.mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'module.model.mask_decoder.transformer.layers.0.norm1.weight', 'module.model.mask_decoder.transformer.layers.0.norm1.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'module.model.mask_decoder.transformer.layers.0.norm2.weight', 'module.model.mask_decoder.transformer.layers.0.norm2.bias', 'module.model.mask_decoder.transformer.layers.0.mlp.layers.0.weight', 'module.model.mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'module.model.mask_decoder.transformer.layers.0.mlp.layers.1.weight', 'module.model.mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'module.model.mask_decoder.transformer.layers.0.norm3.weight', 'module.model.mask_decoder.transformer.layers.0.norm3.bias', 'module.model.mask_decoder.transformer.layers.0.norm4.weight', 'module.model.mask_decoder.transformer.layers.0.norm4.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight', 'module.model.mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'module.model.mask_decoder.transformer.layers.1.self_attn.q_proj.weight', 'module.model.mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'module.model.mask_decoder.transformer.layers.1.self_attn.k_proj.weight', 'module.model.mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'module.model.mask_decoder.transformer.layers.1.self_attn.v_proj.weight', 'module.model.mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'module.model.mask_decoder.transformer.layers.1.self_attn.out_proj.weight', 'module.model.mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'module.model.mask_decoder.transformer.layers.1.norm1.weight', 'module.model.mask_decoder.transformer.layers.1.norm1.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'module.model.mask_decoder.transformer.layers.1.norm2.weight', 'module.model.mask_decoder.transformer.layers.1.norm2.bias', 'module.model.mask_decoder.transformer.layers.1.mlp.layers.0.weight', 'module.model.mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'module.model.mask_decoder.transformer.layers.1.mlp.layers.1.weight', 'module.model.mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'module.model.mask_decoder.transformer.layers.1.norm3.weight', 'module.model.mask_decoder.transformer.layers.1.norm3.bias', 'module.model.mask_decoder.transformer.layers.1.norm4.weight', 'module.model.mask_decoder.transformer.layers.1.norm4.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight', 'module.model.mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'module.model.mask_decoder.transformer.final_attn_token_to_image.q_proj.weight', 'module.model.mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'module.model.mask_decoder.transformer.final_attn_token_to_image.k_proj.weight', 'module.model.mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'module.model.mask_decoder.transformer.final_attn_token_to_image.v_proj.weight', 'module.model.mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'module.model.mask_decoder.transformer.final_attn_token_to_image.out_proj.weight', 'module.model.mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'module.model.mask_decoder.transformer.norm_final_attn.weight', 'module.model.mask_decoder.transformer.norm_final_attn.bias', 'module.model.mask_decoder.query_feat.weight', 'module.model.mask_decoder.query_embed.weight', 'module.model.mask_decoder.output_upscaling.0.weight', 'module.model.mask_decoder.output_upscaling.0.bias', 'module.model.mask_decoder.output_upscaling.1.weight', 'module.model.mask_decoder.output_upscaling.1.bias', 'module.model.mask_decoder.output_upscaling.3.weight', 'module.model.mask_decoder.output_upscaling.3.bias', 'module.model.mask_decoder.conv_s0.weight', 'module.model.mask_decoder.conv_s0.bias', 'module.model.mask_decoder.conv_s1.weight', 'module.model.mask_decoder.conv_s1.bias', 'module.model.mask_decoder.channel_reduction.0.weight', 'module.model.mask_decoder.channel_reduction.0.bias', 'module.model.mask_decoder.class_prediction_head.layers.0.weight', 'module.model.mask_decoder.class_prediction_head.layers.0.bias', 'module.model.mask_decoder.class_prediction_head.layers.1.weight', 'module.model.mask_decoder.class_prediction_head.layers.1.bias', 'module.model.mask_decoder.class_prediction_head.layers.2.weight', 'module.model.mask_decoder.class_prediction_head.layers.2.bias'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from model import InstanceStudent\n",
    "from utils import LargeScaleJittering, instance_inference\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mean = torch.tensor([[[123.6750]], [[116.2800]], [[103.5300]]]).to(DEVICE)\n",
    "std = torch.tensor([[[58.3950]], [[57.1200]], [[57.3750]]]).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "img = Image.open(\"/media/avalocal/T7/pardis/pardis/perception_system/datasets/cityscapes/leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png\")\n",
    "img = np.array(img)\n",
    "# img = cv2.resize(img, (1024, 512))\n",
    "img = torch.tensor(img).permute(2, 0, 1).unsqueeze(0).float()/255.0\n",
    "print(img.max(), img.min())\n",
    "img = TF.resize(img, (512, 1024))\n",
    "img = img.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "# plt.imshow(img[0].permute(1, 2, 0))\n",
    "\n",
    "\n",
    "model =InstanceStudent(\"base_plus\")\n",
    "\n",
    "ckpt = \"/home/avalocal/Desktop/checkpoint_epoch_460.pth\"\n",
    "checkpoint = torch.load(ckpt, map_location=DEVICE, weights_only=True)\n",
    "print(checkpoint['model_state_dict'].keys())\n",
    "checkpoint['model_state_dict'] = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "with torch.amp.autocast(device_type='cuda'):\n",
    "    pred_out = model(img.to(DEVICE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.9359e-05, 0.0000e+00, 0.0000e+00, 1.2201e-05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0132e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0585e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3320e-05, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.7560e-06,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.0407e-05, 0.0000e+00, 1.0020e-05, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1330e-05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.2265e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9651e-05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.1537e-05, 0.0000e+00, 0.0000e+00, 1.4546e-05, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>) this is scores\n",
      "this is idx tensor([], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "pred_out.keys()\n",
    "# pred_out['pred_masks'] = TF.resize(pred_out['pred_masks'], (1024, 2048),  interpolation=TF.InterpolationMode.NEAREST) \n",
    "\n",
    "mask_pred_results = pred_out['pred_masks'] #1, 100, 1024, 2048\n",
    "mask_cls_results = pred_out['pred_logits'] #1, 100, 9\n",
    "\n",
    "del pred_out #just for memory\n",
    "\n",
    "processed_results = []\n",
    "for mask_cls_result, mask_pred_result in zip(\n",
    "    mask_cls_results, mask_pred_results\n",
    "):\n",
    "    instance_result = instance_inference(mask_cls_result, mask_pred_result)\n",
    "    processed_results.append(\n",
    "        {\n",
    "            # \"boxes\": instance_result.pred_boxes.tensor,\n",
    "            \"classes\": instance_result.pred_classes,\n",
    "            \"masks\": instance_result.pred_masks,\n",
    "            \"scores\": instance_result.scores,\n",
    "        }\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
